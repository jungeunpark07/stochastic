{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2406a7df-f282-4b93-8c2b-40f19a124858",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c465ce86-0999-4673-a94b-b34654a032f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm_notebook as tq\n",
    "import tqdm\n",
    "import os, time, math, copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "torch.set_printoptions(precision=8, linewidth=50000)\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c4219-8b88-463d-b57d-dd02e947a64f",
   "metadata": {},
   "source": [
    "# Print Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "056d873c-4a0d-4de1-860d-6f604a48fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLACK\t= '\\033[30m'\n",
    "RED\t\t= '\\033[31m'\n",
    "GREEN\t= '\\033[32m'\n",
    "YELLOW\t= '\\033[33m'\n",
    "BLUE\t= '\\033[34m'\n",
    "MAGENTA\t= '\\033[35m'\n",
    "CYAN\t= '\\033[36m'\n",
    "RESET\t= '\\033[0m'\n",
    "SEL\t\t= '\\033[7m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64058b7-984f-4bd5-bc1c-3297042c62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class\tfxp:\n",
    "\tdef\t__init__(self, bIn, iBWF):\n",
    "\t\tself.iFullBW\t= len(bIn)\n",
    "\t\tself.iIntgBW\t= self.iFullBW - iBWF\n",
    "\t\tself.bSign\t\t= bIn[0]\n",
    "\t\tself.bIntg\t\t= bIn[:self.iIntgBW]\n",
    "\t\tself.bFrac\t\t= bIn[self.iIntgBW:]\n",
    "\t\tself.fFull\t\t= 0\n",
    "\t\ttry:\n",
    "\t\t\tfor idx, bit in enumerate(bIn):\n",
    "\t\t\t\tif\tidx == 0:\n",
    "\t\t\t\t\tself.fFull = self.fFull + int(bit,2) * -pow(2, self.iIntgBW - 1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tself.fFull = self.fFull + int(bit,2) * pow(2, self.iIntgBW - 1 - idx)\n",
    "\t\texcept:\n",
    "\t\t\tprint(bIn)\n",
    "\t\tself.dispFull\t= RED + self.bIntg + BLUE + self.bFrac + RESET\n",
    "\t\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae69422b-795b-4e2b-9b2c-63702c4aa2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class\tflp2fix:\n",
    "\tdef\t__init__(self, fIn, iBW, iBWF):\n",
    "\t\tself.fMin\t\t= - 2 ** (iBW - iBWF - 1)\n",
    "\t\tself.fMax\t\t= (2 ** (iBW-1) - 1) * (2 ** -iBWF)\n",
    "\t\tself.fResol\t\t= 2 ** -iBWF\n",
    "\t\tif fIn < self.fMin or fIn > self.fMax:\n",
    "\t\t\tprint(f'({fIn}): Out of input range ({self.fMax}/{self.fMin}) during flp -> fix converting ')\n",
    "\t\tself.iBW\t\t= iBW\n",
    "\t\tself.iBWI\t\t= iBW - iBWF\n",
    "\t\tself.iBWF\t\t= iBWF\n",
    "\n",
    "\t\tself.iFLP2INT\t= abs(int(fIn * 2 ** iBWF))\n",
    "\t\tif fIn < 0:\n",
    "\t\t\tself.iFLP2INT = 2 ** (iBW-1) - self.iFLP2INT\n",
    "\n",
    "\t\tif fIn >= 0:\n",
    "\t\t\tself.bFull = bin(self.iFLP2INT)[2:].rjust(iBW, '0')\n",
    "\t\telse:\n",
    "\t\t\tself.bFull = '1'+bin(self.iFLP2INT)[2:].rjust(iBW-1, '0')\n",
    "\t\t\tif len(self.bFull) > iBW:\n",
    "\t\t\t\tself.bFull = '0' * iBW\n",
    "\n",
    "\t\tself.cssFxp\t\t= fxp(self.bFull, self.iBWF)\n",
    "\t\tself.bSign\t\t= self.cssFxp.bSign\n",
    "\t\tself.bIntg\t\t= self.cssFxp.bIntg\n",
    "\t\tself.bFrac\t\t= self.cssFxp.bFrac\n",
    "\t\tself.fFull\t\t= self.cssFxp.fFull\n",
    "\t\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d34d648-b811-4ee0-b518-3b10c97e7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def\tflp2fixTensor(fIn, iBW, iBWF):\n",
    "\tfMin = - 2 ** (iBW - iBWF - 1)\n",
    "\tfMax = (2 ** (iBW-1) - 1) * (2 ** -iBWF)\n",
    "\tfList = []\n",
    "\tfor aTensor in fIn.view(-1):\n",
    "\t\tfList.append(flp2fix(aTensor, iBW, iBWF).fFull)\n",
    "\treturn torch.tensor(fList).view(fIn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6562c1-c9bd-428a-9b1d-c19286c0bc9c",
   "metadata": {},
   "source": [
    "# User Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e84144fc-99bb-4e41-bdbf-5349ea1162e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '~/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea932772-49ae-4b5b-bae7-a6188c2f3f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch for MNIST dataset')\n",
    "parser.add_argument('--device', type=str, default='cpu', help='Device')\n",
    "parser.add_argument('--shuffle', action='store_true', default=False, help='enables data shuffle')\n",
    "parser.add_argument('--dataset', type=str, default='mnist', help='training dataset')\n",
    "parser.add_argument('--data_path', type=str, default=data_path, help='path to MNIST')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=10, help='number of epochs to train')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--optimizer', type=str, default='adam', help='optimizer')\n",
    "parser.add_argument('--loss_func', type=str, default='cel', help='optimizer')\n",
    "parser.add_argument('--quant_opt', type=str, default='asym', help='Type of Quantization')\n",
    "parser.add_argument('--full_bits', type=int, default=8, help='Number of Quantization Bits')\n",
    "parser.add_argument('--frac_bits', type=int, default=3, help='Number of Quantization Bits')\n",
    "parser.add_argument('--pretrained', type=bool, default=True, help='Pretrained Model')\n",
    "parser.add_argument('--act_quant', type=bool, default=False, help='Activation Quantization')\n",
    "parser.add_argument('--disp', type=bool, default=False, help='Display Model Information')\n",
    "parser.add_argument('--bBW',type=int,default=4,help='bit number')\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e302edf6-62d7-4b65-9ce5-6a3ac2c94aa0",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cbdfa0b-c8b1-4aaa-8984-cbcca3c5c3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.device == 'cuda' else {}\n",
    "if args.dataset == 'mnist':\n",
    "\ttrain_loader = torch.utils.data.DataLoader(\n",
    "\t\tdataset=datasets.MNIST(\n",
    "\t\t\troot=args.data_path,\n",
    "\t\t\ttrain=True,\n",
    "\t\t\tdownload=True,\n",
    "\t\t\ttransform=transforms.ToTensor()\n",
    "\t\t),\n",
    "\t\tbatch_size=args.batch_size,\n",
    "\t\tshuffle=args.shuffle,\n",
    "\t\t**kwargs\n",
    "\t)\n",
    "\n",
    "\ttest_loader = torch.utils.data.DataLoader(\n",
    "\t\tdataset=datasets.MNIST(\n",
    "\t\t\troot=args.data_path,\n",
    "\t\t\ttrain=False,\n",
    "\t\t\tdownload=True,\n",
    "\t\t\ttransform=transforms.ToTensor()\n",
    "\t\t),\n",
    "\t\tbatch_size=args.batch_size,\n",
    "\t\tshuffle=args.shuffle,\n",
    "\t\t**kwargs\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b293a232-6746-49e9-a243-92a946b05896",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7361cc63-0614-40dc-ada6-d539efe1994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(MLP, self).__init__()\n",
    "\t\tself.flatten = nn.Flatten()\n",
    "\t\tself.fc1 = nn.Linear(28*28, 16)\n",
    "\t\tself.relu1 = nn.ReLU()\n",
    "\t\tself.fc2 = nn.Linear(16, 16)\n",
    "\t\tself.relu2 = nn.ReLU()\n",
    "\t\tself.fc3 = nn.Linear(16, 10)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.flatten(x)\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.relu1(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\tx = self.relu2(x)\n",
    "\t\tlogits = self.fc3(x)\n",
    "\t\treturn logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ed3f09d-808d-489d-a527-860dcf20bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genOptimizer(model, args):\n",
    "\tif args.optimizer == 'sgd':\n",
    "\t\toptimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "\tif args.optimizer == 'adam':\n",
    "\t\toptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\treturn optimizer\n",
    "\n",
    "def genLossFunc(args):\n",
    "\tif args.loss_func == 'cel':\n",
    "\t\tloss_func = nn.CrossEntropyLoss()\n",
    "\treturn loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "348b7f57-d809-4d1b-91ab-64331e739a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, epoch, args):\n",
    "\tmodel.train()\n",
    "\tloss_func = genLossFunc(args)\n",
    "\toptimizer = genOptimizer(model, args)\n",
    "\tmax_batch_index = int(np.floor(len(train_loader.dataset)/args.batch_size))\n",
    "\trunning_loss = 0\n",
    "\tfor batch_index, (image, label) in enumerate(tq(train_loader, desc='Train', leave=False)):\n",
    "\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\tpred = model(image)\n",
    "\t\tloss = loss_func(pred, label)\n",
    "\t\trunning_loss += loss.item()#*image.size(0)\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\t\t\n",
    "\tprint(f'Epoch {epoch+1:<3d}: Avg. Loss: {running_loss/len(train_loader.dataset):.4f}', end = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d67808af-a32c-4c73-a36f-7e8de8b0e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, args):\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tloss_func = genLossFunc(args)\n",
    "\t\tloss, correct = 0, 0\n",
    "# \t\tfor batch_index, (image, label) in enumerate(tq(test_loader, desc='Test', leave=False)):\n",
    "\t\tfor batch_index, (image, label) in enumerate(test_loader):\n",
    "\t\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\t\tpred = model(image)\n",
    "\t\t\tloss += loss_func(pred, label).item()#*image.size(0)\n",
    "\t\t\tcorrect += (pred.argmax(1) == label).type(torch.int).sum().item()\n",
    "\tloss /= len(test_loader.dataset)\n",
    "\tcorrect_rate = 100 * correct / len(test_loader.dataset)\n",
    "\tprint(f'Accuracy: {correct}/{len(test_loader.dataset)} ({correct_rate:>.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6be9b886-0a76-4d8d-baf7-987b4fdec1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model):\n",
    "\tfor epoch in range(args.epochs):\n",
    "\t\ttrain(train_loader, model, epoch, args)\n",
    "\t\ttest(test_loader, model, args)\n",
    "\tprint(\"Done!\")\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32f8f899-6bb3-442a-a879-e204b693a944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9524/10000 (95.2%)\n"
     ]
    }
   ],
   "source": [
    "if args.pretrained:\n",
    "    if os.path.isfile('preTrainedModel.pth'):\n",
    "        model = MLP().to(args.device)\n",
    "        model.load_state_dict(torch.load('preTrainedModel.pth'))\n",
    "        test(test_loader, model, args)\n",
    "    else:\n",
    "        model = main(MLP().to(args.device))\n",
    "        torch.save(model.state_dict(), 'preTrainedModel.pth')\n",
    "else:\n",
    "\tmodel = main(MLP().to(args.device))\n",
    "\ttorch.save(model.state_dict(), 'preTrainedModel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08ebbf45-272c-43f6-890a-9239c05ee74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.02905918,  0.02385001,  0.00032381,  ..., -0.00330848,  0.03122507,  0.01620386],\n",
      "        [ 0.02255142, -0.00389863,  0.00861061,  ...,  0.02175023, -0.03418417, -0.01858932],\n",
      "        [-0.02584666,  0.00479941, -0.02506999,  ..., -0.02010476,  0.01693955,  0.01101075],\n",
      "        ...,\n",
      "        [ 0.03105099, -0.03084483,  0.00437470,  ..., -0.01286467,  0.03370811,  0.02605597],\n",
      "        [-0.03491382, -0.00306286, -0.01362592,  ...,  0.02577536, -0.03532766, -0.01946842],\n",
      "        [ 0.00877646,  0.00609850,  0.01847609,  ...,  0.01105409, -0.02793505,  0.00065047]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37182dc5-06c8-49df-b1f5-f72fbece445a",
   "metadata": {},
   "source": [
    "# SNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad450f97-2553-48dc-9089-a553b1c26a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Comp(a,lfsr,snum):\n",
    "    for com in range(0,len(lfsr)):\n",
    "        oA = '0'\n",
    "        if a[com]!=lfsr[com]:\n",
    "            if(int(a[com]) > int(lfsr[com])):\n",
    "                oA = '1'\n",
    "            break\n",
    "    return XOR(oA,snum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0469a378-c40d-45c4-b1ac-d5a80c295288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snum(a):\n",
    "    if a > 0 :\n",
    "        return '0'\n",
    "    else :\n",
    "        return '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74eced07-5e7a-4bf7-9e7b-e913117dd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFSR:\n",
    "    def Random(self):\n",
    "        self.b0 = eval(f'str(random.randint(0,1))')\n",
    "        self.b1 = eval(f'str(random.randint(0,1))')\n",
    "        self.b2 = eval(f'str(random.randint(0,1))')\n",
    "        self.b3 = eval(f'str(random.randint(0,1))')\n",
    "        \n",
    "        return self.b0 + self.b1 + self.b2 + self.b3\n",
    "    \n",
    "    def Normal(self,stream):\n",
    "        self.b0 = XOR(int(stream[2]),int(stream[3]))\n",
    "        self.b1 = stream[0]\n",
    "        self.b2 = stream[1]\n",
    "        self.b3 = stream[2]\n",
    "        \n",
    "        return self.b0 + self.b1 + self.b2 + self.b3\n",
    "    \n",
    "    def Allzero(self):\n",
    "        self.b0 = '0'\n",
    "        self.b1 = '0'\n",
    "        self.b2 = '0'\n",
    "        self.b3 = '0'\n",
    "        \n",
    "        return self.b0 + self.b1 + self.b2 + self.b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f04bb633-1c97-472e-a596-cc445bcb5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR(iA,iB):\n",
    "    if iA != iB :\n",
    "        iOut = '1'\n",
    "    else :\n",
    "        iOut = '0'\n",
    "    return iOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7228b31a-6b0b-4c39-955f-ff013af6f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNG(iIN):\n",
    "\n",
    "    sNUM = snum(iIN)\n",
    "    \n",
    "    bIN = flp2fix(iIN,args.full_bits,args.frac_bits).bFull\n",
    "    oAlist = []\n",
    "    #lfsrlist = []\n",
    "    \n",
    "    lfsr = LFSR()\n",
    "    \n",
    "    for k in range(2**(args.bBW)): #lfsr number generating\n",
    "        if k == 0:\n",
    "            lNUM = lfsr.Random()\n",
    "        else :\n",
    "            lNUM = lfsr.Normal(lNUM)\n",
    "        if (k == 2**(args.bBW)-1):\n",
    "            lNUM = lfsr.Allzero()\n",
    "        \n",
    "        if len(lNUM) != len(bIN[-(args.frac_bits):]) :\n",
    "            if len(lNUM) < len(bIN[-(args.frac_bits):]) :\n",
    "                lNUM = lNUM + (len(bIN[-(args.frac_bits):])-len(lNUM))*'0'\n",
    "            else :\n",
    "                print(\"it can't work\")\n",
    "                return 0\n",
    "        oAlist.append(Comp(bIN[-(args.frac_bits):],lNUM,sNUM)) #comparator of input a\n",
    "        \n",
    "    oAnum = oAlist.count('1')\n",
    "    \n",
    "    oAlist.insert(0,sNUM)\n",
    "    sA = \"\".join(oAlist)\n",
    "    return sA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10217386-de1f-4517-a95d-20470789a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNGTensor(fIn):\n",
    "    sList = []\n",
    "    for aTensor in fIn.view(-1):\n",
    "        sList.append(SNG(float(aTensor)))\n",
    "                     \n",
    "    return torch.tensor(sList).view(fIn.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64f100-1744-4e06-a386-a6059781e6f4",
   "metadata": {},
   "source": [
    "## Find max, min "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d38c9f7f-f3ab-4d1b-95db-941091645fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findActMaxMin():\n",
    "    SF = {}\n",
    "    for i in range(3):\n",
    "            max = torch.max(eval(f'act{i}'))\n",
    "            min = torch.min(eval(f'act{i}'))\n",
    "            exec(f\"SF['act{i}']=torch.max(abs(max),abs(min)).item()\")\n",
    "    return SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b91a2df7-b594-4a6a-a179-595222e5a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWeightMaxMin(model, args):\n",
    "    SF = {}\n",
    "    for name, data in model.named_parameters():\n",
    "        if \"fc\" and \"weight\" in name :\n",
    "            max = torch.max(data)\n",
    "            min = torch.min(data)\n",
    "            exec(f\"SF['{name}']=torch.max(abs(max),abs(min)).item()\")\n",
    "    return SF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1828b2-ac9c-463b-a05c-c2945674eecd",
   "metadata": {},
   "source": [
    "## Fixed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d113cc44-8936-42f4-9edc-d5a9500c3ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2fix(model, args):\n",
    "\tfor name, _ in model.named_parameters():\n",
    "\t\texec(f'model.{name}.data = flp2fixTensor(model.{name}.data, {args.full_bits}, {args.frac_bits})')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "450e4437-1f3c-4976-bf36-f66da4b8fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantFixForward(model, x, args):\n",
    "\tcmodel = copy.deepcopy(model).to(args.device)\n",
    "\twith torch.no_grad():\n",
    "\t\tx = cmodel.flatten(x)\n",
    "\t\tact0 = flp2fixTensor(x, args.full_bits, args.frac_bits)\n",
    "\t\t\n",
    "\t\tact1 = model.relu1(cmodel.fc1(act0))\n",
    "\t\tact1 = flp2fixTensor(act1, args.full_bits, args.frac_bits)\n",
    "\n",
    "\t\tact2 = cmodel.relu2(cmodel.fc2(act1))\n",
    "\t\tact2 = flp2fixTensor(act2, args.full_bits, args.frac_bits)\n",
    "\t\t\n",
    "\t\tact3 = cmodel.fc3(act2)\n",
    "\t\tact3 = flp2fixTensor(act3, args.full_bits, args.frac_bits)\n",
    "\treturn cmodel, act0, act1, act2, act3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a41e1a82-82ee-42fa-9f7f-8ff0981e3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQuant(model, test_loader, args):\n",
    "\tqmodel = copy.deepcopy(model).to(args.device)\n",
    "\tqmodel = model2fix(qmodel, args)\n",
    "\tqmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tloss_func = genLossFunc(args)\n",
    "\t\tloss, correct = 0, 0\n",
    "\t\tfor batch_index, (image, label) in enumerate(tq(test_loader,desc='Test',leave=False)):\n",
    "\t\t\timage, label = image.to(args.device), label.to(args.device)\n",
    "\t\t\tqmodel, act0, act1, act2, act3  = quantFixForward(qmodel, image, args)\n",
    "\t\t\ty = act3\n",
    "\t\t\tloss += loss_func(y, label).item()#*image.size(0)\n",
    "\t\t\tcorrect += (y.argmax(1) == label).type(torch.int).sum().item()\n",
    "\tcorrect_rate = 100 * correct / len(test_loader.dataset)\n",
    "\tprint(f'Accuracy: {correct}/{len(test_loader.dataset)} ({correct_rate:>.1f}%) Loss: {loss/len(test_loader.dataset):.2f}')\n",
    "\treturn qmodel, act0, act1, act2, act3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "824c658d-8ac0-41cf-afec-21c721f67ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17.4375): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(15.890625): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(-16.75): Out of input range (15.875/-16) during flp -> fix converting \n",
      "1000b110\n",
      "(17.15625): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(17.84375): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(16.453125): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(15.984375): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(17.421875): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(17.0): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(16.875): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(16.3125): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(18.109375): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(16.46875): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(18.46875): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(19.78125): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(18.09375): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(16.890625): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(16.65625): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(16.953125): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(18.265625): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(17.125): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(18.15625): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(16.125): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(17.296875): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(16.578125): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(15.96875): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(21.546875): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(15.96875): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(17.796875): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(16.0): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(17.5): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(16.390625): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(17.140625): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(18.203125): Out of input range (15.875/-16) during flp -> fix converting \n",
      "(16.0): Out of input range (15.875/-16) during flp -> fix converting \n",
      "Accuracy: 8270/10000 (82.7%) Loss: 0.01\n"
     ]
    }
   ],
   "source": [
    "qmodel, act0, act1, act2, act3 = testQuant(model, test_loader, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5038a8cb-900f-4591-a0df-c02136a72476",
   "metadata": {},
   "source": [
    "## apply SNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "168f21d0-24e1-47ae-9acb-7b64ec806a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def N2S(qmodel,act0,act1,act2,args):\n",
    "    aSF = findActMaxMin()\n",
    "    wSF = findWeightMaxMin(qmodel,args)\n",
    "    \n",
    "    a0 = SNGTensor(act0/aSF['act0'])\n",
    "    a1 = SNGTesnor(act1/aSF['act1'])\n",
    "    a2 = SNGTensor(act2/aSF['act2'])\n",
    "    \n",
    "    w0 = SNGTensor(qmodel.fc1.weight/wSF['fc1.weight'])\n",
    "    w1 = SNGTensor(qmodel.fc2.weight/wSF['fc2.weight'])\n",
    "    w2 = SNGTensor(qmodel.fc3.weight/wSF['fc3.weight'])\n",
    "    \n",
    "    return a0,a1,a2,w0,w1,w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84f69eaf-0bb5-4dbd-a71a-7e1a550e8304",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d9a7426d6343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN2S\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-a07d322d3e95>\u001b[0m in \u001b[0;36mN2S\u001b[0;34m(qmodel, act0, act1, act2, args)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mwSF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindWeightMaxMin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0ma0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSNGTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0maSF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'act0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSNGTesnor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0maSF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'act1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSNGTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0maSF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'act2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-d80ec13a0cad>\u001b[0m in \u001b[0;36mSNGTensor\u001b[0;34m(fIn)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSNG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfIn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "a0,a1,a2,w0,w1,w2 = N2S(qmodel,act0,act1,act2,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6895c6a-e780-4242-a121-6538af392eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
